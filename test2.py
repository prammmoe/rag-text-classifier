# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ae1ZP8TLq0N4vajHrcQfs-btLXkBDXf4
"""

import os
import glob
import signal
import sys
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# Load all PDFs from the specified folder
pdf_folder_path = "/content/drive/MyDrive/TSDN 2024/BARU/Data/"  # Update to your path
all_pdf_paths = glob.glob(os.path.join(pdf_folder_path, "*.pdf"))

# Load each PDF document and split text
documents = []
for pdf_path in all_pdf_paths:
    loader = PyPDFLoader(pdf_path)
    pdf_docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    documents.extend(text_splitter.split_documents(pdf_docs))

print(f"Total loaded document chunks: {len(documents)}")

# Set up embeddings and LLM with Google Gemini API
GEMINI_API_KEY = "AIzaSyDFQrUxPXyeVGU66oxymNMeK9IZy_Z272U"
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GEMINI_API_KEY)
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=GEMINI_API_KEY)

# Create FAISS vector database from documents
vector_db = FAISS.from_documents(documents, embeddings)
retriever = vector_db.as_retriever(search_type="similarity", search_kwargs={"k": 5})

# Definisikan template RAG prompt dalam bahasa Indonesia
def generate_rag_prompt(query, context):
    prompt = f"""
Anda adalah bot yang berperan sebagai ahli kesehatan dan medis yang dapat memberikan jawaban informatif berdasarkan konteks teks yang tersedia di bawah ini.
Jawablah pertanyaan dengan kalimat lengkap yang menyeluruh, mencakup semua informasi latar belakang yang relevan.
Ingat, Anda berbicara kepada audiens non-teknis, jadi jelaskan konsep yang rumit dengan cara yang sederhana dan gunakan nada yang ramah.
PERTANYAAN: '{query}'
KONTEKS: '{context}'
JAWABAN:
"""
    return prompt

# Buat template dalam bahasa Indonesia untuk chain RetrievalQA
template = """
Anda adalah seorang ahli medis dan kesehatan yang berpengalaman dalam menjelaskan jawaban akurat dari teks yang kompleks.
Manfaatkan konteks yang diberikan untuk memberikan jawaban yang jelas dan terinci.

Konteks:
{context}

Berikan jawaban yang informatif dan mendalam berdasarkan konteks yang ada:
"""

# Buat prompt template
prompt = ChatPromptTemplate.from_messages([("system", template), ("human", "{input}")])

# Create RetrievalQA chain with specified chain_type and prompt
from langchain.chains import RetrievalQA  # Import RetrievalQA
chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": prompt})

# Handle interrupt signal for clean exit
def signal_handler(sig, frame):
    print('\nTerima Kasih Telah Menggunakan RAG LLM Chatbot. :)')
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)

from langchain.schema import HumanMessage

while True:
    query = input("Query (or type 'exit' to quit): ")
    if query.lower() == 'exit':  # Check if the user wants to exit
        print("Exiting the program. Goodbye!")
        break  # Exit the loop

    context = "\n".join([result.page_content for result in retriever.get_relevant_documents(query)])
    prompt = generate_rag_prompt(query=query, context=context)

    # Create a HumanMessage object with the generated prompt
    messages = [HumanMessage(content=prompt)]

    # Pass the messages to the llm
    answer = llm(messages=messages)
    print("Answer:", answer)